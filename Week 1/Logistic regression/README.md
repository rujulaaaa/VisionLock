# Logistic Regression

>We just survived **Linear Regression**, the longest and most conceptual part of classical ML.  
Good news? Logistic regression is shorter and much easier than linear one, so don’t worry, we’re almost through the heavy stuff!

---

If you want a gentle intuition first, you may read this **article**: [Logistic Regression Intuition](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html) *(Optional)*
>What's better is- you could read this once. Try to understand, relate to linear regression to an extent (even if you do not understand all of it). Then take a break of few hours. Watch all the videos that follow and come back here to revise.

Here's the link: **[Logistic Regression + Regularization](https://youtube.com/playlist?list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI&si=AhpQF-nmu1iibsVH)**

1. Logistic Regression (Videos 32 – 36)
- Why LR is used for classification
- Sigmoid function  
- Decision boundaries  

2. Regularisation (Videos 37 – 39, 41)  
 > Just one more little part, I am sure you can do it. Please don't skip it or leave it for later. It is important.

3. Multinomial Logistic Regression: Refer to this article- [Multinomial Logistic Regression](https://www.quarkml.com/2022/03/multinomial-logistic-regression-definition-math-and-implementation.html)

---

## What's Next?
Now it’s time to apply what you learned — head over to the **[Assignments]()** and give them a shot. Struggle a bit, experiment a bit, and most importantly, enjoy the process. I’m here if you get stuck anywhere.



